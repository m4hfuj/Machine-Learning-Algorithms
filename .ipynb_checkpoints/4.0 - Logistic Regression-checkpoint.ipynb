{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16c6d6b1-112d-40c0-8339-cd4020ba3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaec3da3-7d3e-45aa-ab97-a71e7218d890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cgpa</th>\n",
       "      <th>placement_exam_marks</th>\n",
       "      <th>placed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.19</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.46</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.54</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.23</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cgpa  placement_exam_marks  placed\n",
       "0  7.19                  26.0       1\n",
       "1  7.46                  38.0       1\n",
       "2  7.54                  40.0       1\n",
       "3  6.42                   8.0       1\n",
       "4  7.23                  17.0       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Datasets/placement.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07b96a07-486e-429c-9fdc-c246d3fbd5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, [0,1]].to_numpy()\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "y = data.iloc[:, 2].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4124d9-1a8e-49bf-9210-1029cd919e41",
   "metadata": {},
   "source": [
    "The logistic loss for a single training example is defined as:\n",
    "\n",
    "$$ \\text{LogisticLoss}(h_\\theta(x), y) = -y \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x)) $$\n",
    "\n",
    "The overall logistic regression cost function is given by:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\text{LogisticLoss}(h_\\theta(x^{(i)}), y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d48190a1-2e28-4f32-9d89-bca079b865e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y, h):\n",
    "    return -y * np.log(h) - (1 - y) * np.log(1 - h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4332f-b982-43fa-91f8-c6e177868482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74850f9d-5b52-4b63-9634-d903e211030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5715e-f739-41a4-8347-012fcd909d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b5d219b-0f84-473d-9976-5c9775201e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'logistic_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m (y_hat \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# grad = np.dot(X.T, (y_hat - y)) / len(y)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# theta = theta - alpha * grad\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogistic_loss\u001b[49m()\n\u001b[1;32m     20\u001b[0m cost \u001b[38;5;241m=\u001b[39m logistic_loss(y, y_hat)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(cost)\n",
      "File \u001b[0;32m~/env_base/lib/python3.10/site-packages/torch/__init__.py:1932\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1932\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'logistic_loss'"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "m = len(y)\n",
    "alpha = 0.001\n",
    "theta = np.ones(X.shape[1])\n",
    "costs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    y_hat = np.dot(X, theta)\n",
    "    y_hat = sigmoid(y_hat)\n",
    "    y_hat = (y_hat >= 0.5).astype(int)\n",
    "\n",
    "    # grad = np.dot(X.T, (y_hat - y)) / len(y)\n",
    "\n",
    "    # theta = theta - alpha * grad\n",
    "    \n",
    "    cost = logistic_loss(y, y_hat)\n",
    "\n",
    "    print(cost)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39173d4-643b-40ae-a1e2-46346c1996fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
